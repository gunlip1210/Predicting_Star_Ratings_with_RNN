{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "umukHleUK1q6"
   },
   "source": [
    "[005] uniform cross entropy.\n",
    "\n",
    "[006] weighted cross entropy.\n",
    "\n",
    "[007] adaptive cross entropy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0LFU5rVfW7Na"
   },
   "source": [
    "# 0. 제반 환경 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t_aqbskLMVF4"
   },
   "outputs": [],
   "source": [
    "# 구글 드라이브 연결.\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "MODEL_PATH = '/model006.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qi0GnFAwS5Zx"
   },
   "outputs": [],
   "source": [
    "# 데이터처리 관련 라이브러리.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re  # 정규표현식 사용 환경 제공.\n",
    "import string\n",
    "from collections import Counter\n",
    "# import seaborn as sns\n",
    "\n",
    "# 자연어처리 관련 라이브러리.\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# 딥러닝 관련 라이브러리.\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm  # .ipynb에 최적화된 프로그래스 바.\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# 데이터셋 로딩용 라이브러리.\n",
    "!pip install -q datasets\n",
    "\n",
    "# LR sceduler 사용을 위한 hugging face의 transformers 설치.\n",
    "!pip install -q transformers\n",
    "\n",
    "# LR sceduler import.\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "# Set pytorch random seed.\n",
    "seed = 1210\n",
    "torch.manual_seed(seed)\n",
    "print(\"Random Seed:\", seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oViPcfEBWvT6"
   },
   "source": [
    "# 1. Data 불러오기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k8uonsW1N9zv"
   },
   "outputs": [],
   "source": [
    "# 데이터셋 로딩.\n",
    "from datasets import load_dataset  # Huggingface의 datasets 라이브러리를 이용.\n",
    "\n",
    "# Yelp Review Full 데이터셋 불러오기.\n",
    "# 음식점, 서비스, 장소 등 비즈니스에 대한 텍스트 후기와 별점(0-4)이 포함된 데이터셋.\n",
    "dataset_train = load_dataset(\"yelp_review_full\", split=\"train\")\n",
    "dataset_test = load_dataset(\"yelp_review_full\", split=\"test\")\n",
    "\n",
    "# 데이터셋의 예시 확인.\n",
    "print(dataset_train)\n",
    "for i in range(10):\n",
    "    print(dataset_train[i])\n",
    "print(dataset_test)\n",
    "for i in range(10):\n",
    "    print(dataset_test[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F5A-VAEdVVg1"
   },
   "outputs": [],
   "source": [
    "# Pandas DataFrame 형태로 변환.\n",
    "train_df = pd.DataFrame(dataset_train)\n",
    "test_df = pd.DataFrame(dataset_test)\n",
    "\n",
    "# train_df와 test_df를 하나의 DataFrame으로 세로로 연결.\n",
    "data_df = pd.concat([train_df, test_df], ignore_index=True)\n",
    "\n",
    "\n",
    "# Train data 65만개, Test data 5만개, 총 70만개, 별점 별 14만개.\n",
    "# 별점 별 동일 개수.\n",
    "data_count = [[data_df.label.value_counts()[0],\n",
    "        data_df.label.value_counts()[1],\n",
    "        data_df.label.value_counts()[2],\n",
    "        data_df.label.value_counts()[3],\n",
    "        data_df.label.value_counts()[4]]]\n",
    "print(\"# of all data:\", data_df.shape[0])\n",
    "pd.DataFrame(data_count, index=[\"# of data\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hsrsby6lXEZE"
   },
   "source": [
    "# 2. Data 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qO72-3LDbmGq"
   },
   "source": [
    "## 2-a. Data Cleaning.\n",
    "unicode characters, puncutation, stopwords 제거."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HCEkd-NCWOwH"
   },
   "outputs": [],
   "source": [
    "def data_preprocessing(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"(@\\[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \" \", text) # Removing Unicode Characters\n",
    "    text = ''.join([c for c in text if c not in string.punctuation]) # Remove punctuation\n",
    "\n",
    "    # Removing Stopwords\n",
    "    # stop words are common words within sentences that do not add value\n",
    "    # and thus can be eliminated when cleaning for NLP prior to analysis.\n",
    "    # e.g., 'the', 'was'\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = [word for word in text.split() if word not in stop_words]\n",
    "    text = ' '.join(text)\n",
    "\n",
    "    return text\n",
    "\n",
    "tqdm.pandas()  # tqdm을 이용하여 진행률 표시하기 위해 초기화.\n",
    "data_df['cleaned_text'] = data_df['text'].progress_apply(data_preprocessing)  # cleaned_text 열을 생성하면서 진행률 표시.\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-OlS5BCxeRbl"
   },
   "source": [
    "# 3. Data 토큰화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3W5nLzoVeuf0"
   },
   "source": [
    "## 3-a. Create Vocab-Int mapping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "20jAtGcKfwvs"
   },
   "source": [
    "단어 추출 후, 빈도를 계산하여 순서대로 *sorted_words*에 저장."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fMC57pDUemuU"
   },
   "outputs": [],
   "source": [
    "corpus = [word for text in data_df['cleaned_text'] for word in text.split()]\n",
    "count_words = Counter(corpus)\n",
    "sorted_words = count_words.most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SfllwH2rf2Zz"
   },
   "source": [
    "Vocabulary 생성.\n",
    "One-hot vectoer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NFkD4R6ZgVkA"
   },
   "outputs": [],
   "source": [
    "vocab_to_int = {w:i+1 for i, (w,c) in enumerate(sorted_words)}\n",
    "\n",
    "# tqdm을 사용하여 cleaned_text 열의 각 텍스트에 대한 진행률 표시.\n",
    "text_int = []\n",
    "\n",
    "# tqdm을 이용해 cleaned_text의 각 항목에 대해 진행률 표시.\n",
    "for text in tqdm(data_df['cleaned_text'], desc=\"Converting text to integers\"):\n",
    "    r = [vocab_to_int[word] for word in text.split()]\n",
    "    text_int.append(r)\n",
    "\n",
    "# 첫 번째 텍스트의 정수 표현 확인.\n",
    "print(text_int[:1])\n",
    "\n",
    "# DataFrame에 정수 표현을 추가.\n",
    "data_df['text_int'] = text_int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PXRsHVIPg8Bg"
   },
   "source": [
    "# 4. Padding and Truncating the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E74qg4C7kU-G"
   },
   "source": [
    "## 4-a. Analyze the text length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fMzmhkPvkW8S"
   },
   "outputs": [],
   "source": [
    "text_len = [len(x) for x in text_int]\n",
    "data_df['text_len'] = text_len\n",
    "\n",
    "print(data_df[\"text_len\"].describe())\n",
    "\n",
    "data_df[\"text_len\"].hist(bins = range(min(data_df[\"text_len\"]), max(data_df[\"text_len\"]) + 50, 50))\n",
    "plt.title('Text length distribution', size=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "03_HDabDkv6-"
   },
   "source": [
    "## 4-b. Padding / Truncation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rw2MAXxBlS52"
   },
   "outputs": [],
   "source": [
    "def Padding(review_int, seq_len):\n",
    "    '''\n",
    "    0으로 패딩\n",
    "    Return features of text_int, where each text is padded with 0's or truncated to the input seq_length.\n",
    "    '''\n",
    "    # Initialize the features array with zeros.\n",
    "    features = np.zeros((len(review_int), seq_len), dtype=int)\n",
    "\n",
    "    # tqdm을 사용하여 패딩 작업의 진행률 표시.\n",
    "    for i, text in tqdm(enumerate(review_int), total=len(review_int), desc=\"Padding reviews\"):\n",
    "        if len(text) <= seq_len:\n",
    "            zeros = list(np.zeros(seq_len - len(text)))\n",
    "            new = text + zeros\n",
    "        else:\n",
    "            new = text[:seq_len]\n",
    "        features[i, :] = np.array(new)\n",
    "\n",
    "    return features\n",
    "\n",
    "# seq_len 200으로 설정하고 Padding 함수 호출.\n",
    "features = Padding(text_int, 200)\n",
    "# print(features[0, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QocR7ZjQlwhn"
   },
   "source": [
    "# 5. Data split\n",
    "test : valid : test = 12 : 1 : 1 비율로 dataset 분리."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ivH7aK4nmorK"
   },
   "outputs": [],
   "source": [
    "# Train set 분리\n",
    "X_train, X_remain, y_train, y_remain = train_test_split(features, data_df['label'].to_numpy(), test_size=1/7, random_state=1)\n",
    "\n",
    "# Valid set, Test set 분리\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_remain, y_remain, test_size=0.5, random_state=1)\n",
    "\n",
    "print(\"# of Train dataset:\", len(X_train))\n",
    "print(\"# of Valid dataset:\", len(X_valid))\n",
    "print(\"# of Test dataset:\", len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "695kEuL6ozWb"
   },
   "source": [
    "# 6. Set hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1dc2dx48o4bB"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64  # 16, 32, 64, 128 등 해보고 결과 비교.\n",
    "\n",
    "EMBED_DIM = 64\n",
    "HIDDEN_DIM = 256\n",
    "N_LAYERS = 4\n",
    "DROPOUT_PROB = 0.3  # 0.3 - 0.5 정도. 데이터셋이 많을 수록 작은 값 사용하는 편.\n",
    "\n",
    "SEMI_DIFF_SCORE = 0.2  # predict label과 real label이 1만큼 차이 날 때, 몇 점으로 매길 것인지. 완전 불일치 0. 완전 일치 1.\n",
    "\n",
    "VOCAB_SIZE = len(vocab_to_int) + 1  # 고정.\n",
    "OUTPUT_SIZE = 5  # 고정."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BErVpINqpDH_"
   },
   "source": [
    "# 7. Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HDnWNoLcpuAc"
   },
   "outputs": [],
   "source": [
    "# trun our data into tensor dataset\n",
    "train_data = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n",
    "test_data = TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test))\n",
    "valid_data = TensorDataset(torch.from_numpy(X_valid), torch.from_numpy(y_valid))\n",
    "\n",
    "# build dataloaders\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=BATCH_SIZE, drop_last=True) # drop_last. 마지막 배치에서 데이터셋이 부족하면 무시.\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=BATCH_SIZE, drop_last=True)\n",
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=BATCH_SIZE, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H02bDzs4NCyB"
   },
   "source": [
    "# 8. Deep learning Model\n",
    "RNN with BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tHj2Lnl9NK2-"
   },
   "outputs": [],
   "source": [
    "# GPU vs. CPU 선택.\n",
    "is_cuda = torch.cuda.is_available()\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, use CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mz9GXfCmOEks"
   },
   "outputs": [],
   "source": [
    "# Model Block 정의\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob):\n",
    "        super().__init__()\n",
    "        self.output_size = output_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        # Embedding and LSTM layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True, bidirectional=True)\n",
    "\n",
    "        # Dropout and fully connected layer\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        # self.softmax = nn.Softmax(dim=1)  # Softmax for multi-class output\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # Embedding and LSTM output\n",
    "        embeds = self.embedding(x)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "\n",
    "        # Stack up LSTM outputs\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "\n",
    "        # Dropout and fully connected layer\n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        # Apply softmax to output layer for multi-class classification\n",
    "        # softmax_out = self.softmax(out)\n",
    "\n",
    "        # Reshape to batch size\n",
    "        # softmax_out = softmax_out.view(batch_size, -1, self.output_size)\n",
    "        # softmax_out = softmax_out[:, -1]  # Select final time step's output\n",
    "\n",
    "        # return softmax_out, hidden\n",
    "\n",
    "        # Softmax 제거 버전.\n",
    "        # Reshape to batch size\n",
    "        out = out.view(batch_size, -1, self.output_size)\n",
    "        out = out[:, -1]  # Select final time step's output\n",
    "\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        h0 = torch.zeros((self.n_layers*2, batch_size, self.hidden_dim)).to(device)  # *2 Bidirection 이므로\n",
    "        c0 = torch.zeros((self.n_layers*2, batch_size, self.hidden_dim)).to(device)  # *2 Bidirection 이므로\n",
    "        hidden = (h0, c0)\n",
    "        return hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "78zMc07eQe9p"
   },
   "outputs": [],
   "source": [
    "# Model 생성.\n",
    "model = LSTM(VOCAB_SIZE, OUTPUT_SIZE, EMBED_DIM, HIDDEN_DIM, N_LAYERS, DROPOUT_PROB)\n",
    "model = model.to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vgIYHO-3Q77k"
   },
   "source": [
    "# 9. Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KIdyPffzSo2X"
   },
   "source": [
    "## 9-a. 학습 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l-K6L2D5VlHY"
   },
   "outputs": [],
   "source": [
    "class WeightedCrossEntropyLoss(nn.Module):\n",
    "    def __init__(self, alpha=1.0, exponent=2):\n",
    "        super(WeightedCrossEntropyLoss, self).__init__()\n",
    "        self.cross_entropy = nn.CrossEntropyLoss(reduction='none')\n",
    "        self.alpha = alpha\n",
    "        self.exponent = exponent\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        # 기본 CrossEntropy 손실 계산 (각 샘플마다)\n",
    "        ce_loss = self.cross_entropy(pred, target)\n",
    "        \n",
    "        # 예측 클래스 추출\n",
    "        _, pred_classes = torch.max(pred, dim=1)\n",
    "        \n",
    "        # 예측 클래스와 실제 클래스의 차이 계산\n",
    "        difference = torch.abs(pred_classes - target)\n",
    "        \n",
    "        # 비선형 가중치 적용 (차이에 대해 거듭제곱과 alpha 가중치 적용)\n",
    "        weights = (self.alpha * (difference.float() ** self.exponent)).detach()\n",
    "        \n",
    "        # 가중치가 적용된 손실\n",
    "        weighted_loss = ce_loss * weights\n",
    "        \n",
    "        # 평균 손실 반환\n",
    "        return weighted_loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zhJ4SnybTj-f"
   },
   "outputs": [],
   "source": [
    "lr = 0.0001  # learning rate.\n",
    "wd = 0.01  # weight decay.\n",
    "loss_func = nn.CrossEntropyLoss()  # 다중 클래스 분류를 위한 손실 함수.\n",
    "custom_loss_func = WeightedCrossEntropyLoss()    # 다중 클래스 분류를 위한 손실 함수. 가중치 고려.\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
    "clip = 5 # for ameliorating the exploding-gradient problem.\n",
    "epochs = 15\n",
    "\n",
    "total_steps = len(train_loader) * epochs\n",
    "warmup_steps = int(0.15 * total_steps)  # 예: 15%를 warmup 단계로 설정\n",
    "\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=warmup_steps,\n",
    "    num_training_steps=total_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HXPYNKpjRF8k"
   },
   "outputs": [],
   "source": [
    "def acc(pred, label):\n",
    "    # 각 샘플에 대해 가장 높은 확률을 가진 클래스를 예측 클래스로 변환\n",
    "    _, pred_classes = torch.max(pred, dim=1)  # 예측 확률의 최대값 인덱스를 클래스 레이블로 변환\n",
    "    correct = torch.sum(pred_classes == label).item()  # 예측이 실제 레이블과 일치하는 수를 계산\n",
    "    return correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vMKuTqN2IdAq"
   },
   "outputs": [],
   "source": [
    "def custom_acc(pred, label):\n",
    "    # 각 샘플에 대해 가장 높은 확률을 가진 클래스를 예측 클래스로 변환\n",
    "    _, pred_classes = torch.max(pred, dim=1)  # 예측 확률의 최대값 인덱스를 클래스 레이블로 변환\n",
    "\n",
    "    # 예측과 실제 레이블 간의 차이를 계산\n",
    "    difference = torch.abs(pred_classes - label)\n",
    "\n",
    "    # 정확하게 일치하면 1, 1만큼 차이나면 0.3, 그 외에는 0 점수 부여\n",
    "    scores = torch.where(difference == 0, 1.0, torch.where(difference == 1, SEMI_DIFF_SCORE, 0.0))\n",
    "\n",
    "    # 전체 점수의 합계를 계산\n",
    "    total_score = torch.sum(scores).item()\n",
    "\n",
    "    return total_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KkUbwunjSure"
   },
   "outputs": [],
   "source": [
    "each_epoch_tr_loss,each_epoch_vl_loss = [],[]\n",
    "each_epoch_tr_acc,each_epoch_vl_acc = [],[]\n",
    "each_epoch_tr_custom_acc,each_epoch_vl_custom_acc = [],[]\n",
    "each_epoch_lr = []\n",
    "valid_loss_min = np.Inf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O9KU3PpjS_Y1"
   },
   "source": [
    "## 9-b. 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uTxD1vU5TCOd"
   },
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    print(\"Epoch\", epoch+1)\n",
    "\n",
    "    ## TRAINING ##\n",
    "    train_losses = []\n",
    "    train_acc = 0.0\n",
    "    train_custom_acc = 0.0\n",
    "    model.train()  # 모델을 훈련 모드로 설정\n",
    "    h = model.init_hidden(BATCH_SIZE)  # 초기 hidden state 설정\n",
    "\n",
    "    print(\"Train Step\")\n",
    "    for inputs, labels in tqdm(train_loader):\n",
    "        h = tuple([each.data for each in h])  # 이전 hidden state 데이터 유지\n",
    "\n",
    "        inputs, labels = inputs.to(device), labels.to(device)  # GPU 또는 CPU로 이동\n",
    "\n",
    "        model.zero_grad()\n",
    "        output, h = model(inputs, h)\n",
    "\n",
    "        # 손실 계산 및 역전파\n",
    "        # loss = loss_func(output, labels)  # ----------\n",
    "        loss = custom_loss_func(output, labels)\n",
    "        loss.backward()\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "        # 정확도 계산\n",
    "        accuracy = acc(output, labels)\n",
    "        custom_accuracy = custom_acc(output, labels)\n",
    "        train_acc += accuracy\n",
    "        train_custom_acc += custom_accuracy\n",
    "\n",
    "        # exploding gradient 문제를 예방\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        # LR scheduler를 사용한다면,\n",
    "        scheduler.step()\n",
    "\n",
    "    print(\"Valid Step\")\n",
    "    ## EVALUATION ##\n",
    "    val_losses = []\n",
    "    val_acc = 0.0\n",
    "    val_custom_acc = 0.0\n",
    "    model.eval()  # 모델을 평가 모드로 설정\n",
    "    val_h = model.init_hidden(BATCH_SIZE)  # 초기 hidden state 설정\n",
    "\n",
    "    for inputs_v, labels_v in tqdm(valid_loader):\n",
    "        val_h = tuple([each.data for each in val_h])\n",
    "\n",
    "        inputs_v, labels_v = inputs_v.to(device), labels_v.to(device)  # GPU 또는 CPU로 이동\n",
    "\n",
    "        output_v, val_h = model(inputs_v, val_h)\n",
    "        # val_loss = loss_func(output_v, labels_v)  # ----------\n",
    "        val_loss = custom_loss_func(output_v, labels_v)\n",
    "\n",
    "        val_losses.append(val_loss.item())\n",
    "\n",
    "        accuracy = acc(output_v, labels_v)\n",
    "        custom_accuracy = custom_acc(output_v, labels_v)\n",
    "        val_acc += accuracy\n",
    "        val_custom_acc += custom_accuracy\n",
    "\n",
    "    # 평균 손실 및 정확도 계산\n",
    "    epoch_train_loss = np.mean(train_losses)\n",
    "    epoch_val_loss = np.mean(val_losses)\n",
    "    epoch_train_acc = train_acc / len(train_loader.dataset)\n",
    "    epoch_val_acc = val_acc / len(valid_loader.dataset)\n",
    "    epoch_train_custom_acc = train_custom_acc / len(train_loader.dataset)\n",
    "    epoch_val_custom_acc = val_custom_acc / len(valid_loader.dataset)\n",
    "\n",
    "    # 손실 및 정확도 기록\n",
    "    each_epoch_tr_loss.append(epoch_train_loss)\n",
    "    each_epoch_vl_loss.append(epoch_val_loss)\n",
    "    each_epoch_tr_acc.append(epoch_train_acc)\n",
    "    each_epoch_vl_acc.append(epoch_val_acc)\n",
    "    each_epoch_tr_custom_acc.append(epoch_train_custom_acc)\n",
    "    each_epoch_vl_custom_acc.append(epoch_val_custom_acc)\n",
    "    each_epoch_lr.append(scheduler.get_last_lr())\n",
    "\n",
    "    # print(f'Epoch {epoch + 1}')\n",
    "    print(\"learning rate:\", scheduler.get_last_lr())\n",
    "    print(f'train_loss : {epoch_train_loss:.4f} | val_loss : {epoch_val_loss:.4f}')\n",
    "    print(f'train_accuracy : {epoch_train_acc * 100:.2f}% | val_accuracy : {epoch_val_acc * 100:.2f}%')\n",
    "    print(f'train_custom_accuracy : {epoch_train_custom_acc * 100:.2f}% | val_custom_accuracy : {epoch_val_custom_acc * 100:.2f}%')\n",
    "\n",
    "    # 검증 손실이 줄어들면 모델 저장\n",
    "    if epoch_val_loss <= valid_loss_min:\n",
    "        torch.save(model.state_dict(), MODEL_PATH)\n",
    "        print('Validation loss decreased ({:.4f} --> {:.4f}).  Saving model ...'.format(valid_loss_min, epoch_val_loss))\n",
    "        valid_loss_min = epoch_val_loss\n",
    "\n",
    "    print(25 * '==')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TGYrCc0YUyNF"
   },
   "source": [
    "# 10. Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KVoEE6hrUz39"
   },
   "outputs": [],
   "source": [
    "# Get test data loss and accuracy\n",
    "\n",
    "test_losses = []  # track loss\n",
    "num_correct = 0\n",
    "\n",
    "# init hidden state\n",
    "test_h = model.init_hidden(BATCH_SIZE)\n",
    "\n",
    "model.eval()\n",
    "# iterate over test data\n",
    "for inputs, labels in tqdm(test_loader):\n",
    "\n",
    "    # Creating new variables for the hidden state, otherwise\n",
    "    # we'd backprop through the entire training history\n",
    "    test_h = tuple([each.data for each in test_h])\n",
    "\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "    output, test_h = model(inputs, test_h)\n",
    "\n",
    "    # calculate loss\n",
    "    test_loss = loss_func(output, labels)  # CrossEntropyLoss는 logits을 사용하므로 squeeze하지 않음\n",
    "    test_losses.append(test_loss.item())\n",
    "\n",
    "    # convert output probabilities to predicted class (0 to 4)\n",
    "    pred = torch.argmax(output, dim=1)  # 가장 높은 확률을 가진 클래스 인덱스 선택\n",
    "\n",
    "    # compare predictions to true label\n",
    "    correct_tensor = pred.eq(labels.view_as(pred))  # 예측과 실제 라벨 비교\n",
    "    correct = np.squeeze(correct_tensor.cpu().numpy())\n",
    "    num_correct += np.sum(correct)\n",
    "\n",
    "# -- stats! -- ##\n",
    "# avg test loss\n",
    "print(\"Test loss: {:.4f}\".format(np.mean(test_losses)))\n",
    "\n",
    "# accuracy over all test data\n",
    "test_acc = num_correct / len(test_loader.dataset)\n",
    "print(\"Test accuracy: {:.2f}%\".format(test_acc * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F6zPd0fSjdeU"
   },
   "outputs": [],
   "source": [
    "print(each_epoch_tr_loss)\n",
    "print(each_epoch_vl_loss)\n",
    "print(each_epoch_tr_acc)\n",
    "print(each_epoch_vl_acc)\n",
    "print(each_epoch_tr_custom_acc)\n",
    "print(each_epoch_vl_custom_acc)\n",
    "print(each_epoch_lr)\n",
    "\n",
    "# 첫 번째 y축에 손실(loss)을 플로팅\n",
    "fig, ax1 = plt.subplots()\n",
    "ax1.plot(each_epoch_vl_loss, color='blue', marker='o', label=\"Validation Loss\")\n",
    "ax1.set_xlabel(\"Epoch\")\n",
    "ax1.set_ylabel(\"Validation Loss\", color='blue')\n",
    "ax1.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "# 두 번째 y축에 정확도(accuracy)를 플로팅\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(each_epoch_vl_acc, color='red', marker='x', linestyle='--', label=\"Validation Accuracy\")\n",
    "ax2.set_ylabel(\"Validation Accuracy\", color='red')\n",
    "ax2.tick_params(axis='y', labelcolor='red')\n",
    "\n",
    "# 제목 추가\n",
    "plt.title(f\"Validation Loss and Accuracy (lr: {lr}, hidden layer: {HIDDEN_DIM})\")\n",
    "\n",
    "# 그래프 표시\n",
    "fig.tight_layout()  # 겹침 방지\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j7XF-8_eyXR_"
   },
   "source": [
    "## 10-append. 학습된 모델 불러오기."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ep-q09ZJyZf6"
   },
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load(MODEL_PATH))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
